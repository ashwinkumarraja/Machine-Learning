{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team :\n",
    "# Abhinand V Chari - 2017B4A71143H\n",
    "# Ashwin Kumar Raja - 2017B4A70599H\n",
    "# Sajith K - 2017B4A71012H\n",
    "\n",
    "\n",
    "# Problem 1B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_NB(train):\n",
    "    \n",
    "    X_train = []\n",
    "    for i in train:\n",
    "        X_train.append(i[:-1])\n",
    "\n",
    "    vocabulary = formVocabulary(X_train)\n",
    "    spam,ham = bucketTrainingSet(train)\n",
    "\n",
    "#     print(vocabulary)\n",
    "    X_spam = []\n",
    "    X_ham = []\n",
    "    \n",
    "    for i in spam :\n",
    "        X_spam.append(i[:-1])\n",
    "        \n",
    "    for i in ham :\n",
    "        X_ham.append(i[:-1])\n",
    "    \n",
    "    spamVocabulary = formVocabulary(X_spam)\n",
    "    hamVocabulary = formVocabulary(X_ham)\n",
    "#     print(spamVocabulary)\n",
    "    pp_spam = priorProb(spamVocabulary)\n",
    "    pp_ham = priorProb(hamVocabulary)\n",
    "    \n",
    "#     print(pp_spam)\n",
    "#     print(pp_ham)\n",
    "    return pp_spam,pp_ham,vocabulary\n",
    "    \n",
    "\n",
    "def bucketTrainingSet(train):\n",
    "    spam_dataset = []\n",
    "    ham_dataset = []\n",
    "    for i in train:\n",
    "        if i[-1] == \"1\":\n",
    "            spam_dataset.append(i)\n",
    "        else:\n",
    "            ham_dataset.append(i)\n",
    "    \n",
    "#     print(spam_dataset)\n",
    "    \n",
    "#     print(ham_dataset)\n",
    "    return spam_dataset,ham_dataset\n",
    "\n",
    "def priorProb(vocabulary):\n",
    "    totalFreq = 0\n",
    "    for i in vocabulary :\n",
    "        totalFreq += vocabulary[i]\n",
    "    for i in vocabulary :\n",
    "        vocabulary[i]/= totalFreq\n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_NB(test,abs_p_spam,abs_p_ham,vocabulary,pp_spam,pp_ham):\n",
    "    misclassified = 0\n",
    "    \n",
    "    for i in test:\n",
    "\n",
    "        lab = i[-1]\n",
    "        i = i[:-1]\n",
    "        p_spam_i = abs_p_spam\n",
    "        p_ham_i = abs_p_ham\n",
    "\n",
    "        n = len(vocabulary)\n",
    "        test_i = i[:-1]\n",
    "        testVocabulary = formVocabulary([test_i])\n",
    "\n",
    "        for k in testVocabulary:\n",
    "\n",
    "            if k in pp_spam: \n",
    "                p_spam_i *= pp_spam[k]\n",
    "            else :\n",
    "                p_spam_i *= ((testVocabulary[k]+1)/(n + len(vocabulary)))\n",
    "            if k in pp_ham:\n",
    "                p_ham_i  *= pp_ham[k]\n",
    "            else:\n",
    "                p_ham_i  *= ((testVocabulary[k]+1)/(n + len(vocabulary)))\n",
    "\n",
    "        if (isSpam(p_spam_i,p_ham_i) and lab == \"0\") or (not isSpam(p_spam_i,p_ham_i) and lab == \"1\"):\n",
    "            misclassified+=1\n",
    "        \n",
    "    misclassified/=len(test)\n",
    "\n",
    "    accuracy = 1 - misclassified\n",
    "    return accuracy\n",
    "\n",
    "def isSpam(p_spam_i,p_ham_i):\n",
    "    if p_spam_i > p_ham_i:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formVocabulary(dataset):\n",
    "    \n",
    "    clean_dataset = removePunctuations(dataset)\n",
    "#     print(clean_dataset)\n",
    "    tokens = Tokenize(clean_dataset)\n",
    "    tokens = removeStopWords(tokens)\n",
    "    vocabulary = {}\n",
    "    for i in tokens :\n",
    "        if i not in vocabulary:\n",
    "            vocabulary[i] = 0\n",
    "        vocabulary[i]+=1\n",
    "    return vocabulary\n",
    "\n",
    "def Tokenize(dataset):\n",
    "    tokens = []\n",
    "    for i in dataset:\n",
    "        words = i.split(' ')\n",
    "#         print(words)\n",
    "        tokens+=(words)\n",
    "    \n",
    "        \n",
    "#     print(tokens)\n",
    "    return tokens\n",
    "\n",
    "def removePunctuations(dataset):\n",
    "    punctuations = [\"?\",\".\",\";\",\"\\'\",\"\\\\\",\"!\",\",\",\"(\",\")\",\":\"]\n",
    "    final_dataset = []\n",
    "    for i in dataset :\n",
    "        for j in punctuations:\n",
    "            i = i.replace(j,'')\n",
    "            i = i.lower()\n",
    "        final_dataset.append(i)\n",
    "    return final_dataset\n",
    "\n",
    "def removeStopWords(tokens):\n",
    "    stopWords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "    \n",
    "    for i in tokens :\n",
    "        if i in stopWords :\n",
    "            tokens.remove(i)\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbsProb(Y):\n",
    "    abs_p_spam = 0\n",
    "    abs_p_ham = 0\n",
    "\n",
    "    for i in Y:\n",
    "        if i[-1] == \"0\":\n",
    "            abs_p_ham+=1\n",
    "        else:\n",
    "            abs_p_spam+=1\n",
    "\n",
    "    abs_p_spam/=len(Y)\n",
    "    abs_p_ham/=len(Y)\n",
    "    \n",
    "    return abs_p_ham,abs_p_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1\n",
      "Accuracy for Fold  1  is  0.7464788732394366\n",
      "Fold  2\n",
      "Accuracy for Fold  2  is  0.7183098591549295\n",
      "Fold  3\n",
      "Accuracy for Fold  3  is  0.7323943661971831\n",
      "Fold  4\n",
      "Accuracy for Fold  4  is  0.676056338028169\n",
      "Fold  5\n",
      "Accuracy for Fold  5  is  0.7323943661971831\n",
      "Fold  6\n",
      "Accuracy for Fold  6  is  0.647887323943662\n",
      "Fold  7\n",
      "Accuracy for Fold  7  is  0.7394366197183099\n",
      "\n",
      "Overall Accuracy -  0.7132796780684104\n"
     ]
    }
   ],
   "source": [
    "fileReader = open('dataset_NB.txt','r')\n",
    "dataset = fileReader.read().splitlines()\n",
    "fileReader.close()\n",
    "\n",
    "dataset = random.sample(dataset,len(dataset)) \n",
    "\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for i in range(7):\n",
    "    fold_val = math.floor(len(dataset)/7)\n",
    "    test = dataset[i*fold_val:(i+1)*fold_val]\n",
    "    train = dataset[:i*fold_val] + dataset[(i+1)*fold_val:]\n",
    "    train_folds.append(train)\n",
    "    test_folds.append(test)\n",
    "    print(\"Fold \",i+1)\n",
    "#     print(\"Training Test Length \",len(train),\"Test Set Length\",len(test))\n",
    "    \n",
    "          \n",
    "          \n",
    "    abs_p_ham,abs_p_spam = getAbsProb(train)\n",
    "    pp_spam,pp_ham,vocabulary = learn_NB(train)\n",
    "    fold_accuracy = classify_NB(test,abs_p_spam,abs_p_ham,vocabulary,pp_spam,pp_ham)\n",
    "    print(\"Accuracy for Fold \",i+1,\" is \",fold_accuracy)\n",
    "    accuracy+= fold_accuracy\n",
    "accuracy/=7\n",
    "print(\"\\nOverall Accuracy - \" ,accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
